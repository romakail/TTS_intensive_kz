{"cells": [{"cell_type": "markdown", "metadata": {"cellId": "m1gy7yaekrok91zvi3zkk", "execution_id": "158cfe18-9248-4dd8-9d43-ada4b21f9126", "jupyter": {"outputs_hidden": false}}, "source": ["### \u041e\u044a\u044f\u0432\u0434\u0435\u043d\u0438\u0435 \u0442\u043e\u0433\u043e, \u0447\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442\u044c"]}, {"cell_type": "markdown", "metadata": {"cellId": "znc13x53t900h6ur90y3e1"}, "source": ["Acknowledgement: this notebook is inspired by speech course from Andrei Malinin"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "1ycxhfwj7bulu4tx37uut"}, "outputs": [], "source": ["# %pip install -r ../requiremets.txt"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2023-09-16T07:41:36.608247Z", "start_time": "2023-09-16T07:41:36.575732Z"}, "cellId": "w6cjps656jmlyt01x1q3i7", "collapsed": false, "jupyter": {"outputs_hidden": false}}, "outputs": [], "source": ["import os\n", "import requests\n", "from pathlib import Path\n", "from urllib.parse import urlencode\n", "\n", "from IPython.display import display, Audio"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def download_file(public_link):\n", "    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n", "    final_url = base_url + urlencode(dict(public_key=public_link))\n", "    response = requests.get(final_url)\n", "    parse_href = response.json()['href']\n", "\n", "    url = parse_href\n", "    start_filename = url.find('filename=')\n", "    end_filename = url[start_filename:].find('&')\n", "    end_name = start_filename + end_filename\n", "    filename = url[start_filename:end_name][9:]\n", "    download_url = requests.get(url)\n", "    final_link = os.path.join(os.getcwd(), filename)\n", "    with open(final_link, 'wb') as ff:\n", "        ff.write(download_url.content)\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2023-09-16T08:21:32.708191Z", "start_time": "2023-09-16T08:21:32.676383Z"}, "cellId": "c5i3yvpvtkb5xgfl3huw38", "collapsed": false, "jupyter": {"outputs_hidden": false}}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2\n", "%matplotlib inline\n", "\n", "from plotting_utils import *\n", "from transforms import *\n", "from tests import *"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "9ol0yirp2bc5cc0r7ly2k5"}, "outputs": [], "source": ["### To download the file uncomment the following line\n", "\n", "# link_to_archive = \"https://disk.yandex.ru/d/aEqBwRPwpIgJUQ\"\n", "# download_file(link_to_archive)\n", "# !unzip 01_DSP.zip\n", "# !rm 01_DSP.zip\n", "# !mkdir -p ../data\n", "# !mv 01_DSP ../data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2023-09-16T08:21:32.968239Z", "start_time": "2023-09-16T08:21:32.941961Z"}, "cellId": "2871rgjeob5ns6u7za18z", "collapsed": false, "jupyter": {"outputs_hidden": false}}, "outputs": [], "source": ["sample_rate = 22050\n", "data_path = Path(\"../data\")\n", "seminar_name = \"01_DSP\"\n", "samples_path = data_path / seminar_name / \"examples\""]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "8z7mqaitn5k15l8we17hhi"}, "outputs": [], "source": ["def create_examples_dict(directory=\"../data/01_DSP/examples\", sample_rate=sample_rate):\n", "    directory = Path(directory)\n", "\n", "    examples = {}\n", "    for wav_path in directory.iterdir():\n", "        if wav_path.suffix == \".wav\":\n", "            wav, sr = librosa.load(str(wav_path))\n", "            assert sr == sample_rate, f\"Working only with audio of sample_rate {sample_rate}\"\n", "            examples[wav_path.stem] = wav\n", "    return examples\n", "\n", "examples = create_examples_dict(directory=samples_path)\n", "wav = examples[\"babenko\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We will apply our transformation to this wav. Then see how it changes."]}, {"cell_type": "markdown", "metadata": {"cellId": "bdm1d9f8birvk7p6y2rj9g", "execution_id": "031bc0a7-5c4c-45e5-af68-db7629137ae3", "jupyter": {"outputs_hidden": false}}, "source": ["### 1. WAV (Waveform Audio Format)\n", "\n", "Sound is recorded by a microphone. It registers a serie of amplitudes of air-pressure. The frequency, at which microphone records amplitudes is called sample-rate. Typicaly, this frequency can be 16kHz, 22kHz or 48kHz, depending on what quality of audio one aim to record.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["wav_44kHz, orig_sr = librosa.load(str(data_path / seminar_name / \"sample_44kHz.wav\"), sr=44100)\n", "print(f\"Original sample rate: {orig_sr}\")\n", "for target_sr in [44100, 24000, 22050, 16000, 8000, 4000]:\n", "    resampled_wav = librosa.resample(wav_44kHz, orig_sr=orig_sr, target_sr=target_sr)\n", "    print(f\"Resampled SR: {target_sr}\")\n", "    display(Audio(resampled_wav, rate=target_sr))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this seminar we will use samples recorded at 22kHz, which means that each second of recorded sound will contain 22 thousand (22050 actually) amplitude samples. Consider the short audio files below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2023-09-16T08:21:36.128869Z", "start_time": "2023-09-16T08:21:35.265622Z"}, "cellId": "6hlhzqh8uup13q4dk81oyfe", "collapsed": false, "jupyter": {"outputs_hidden": false}}, "outputs": [], "source": ["print(f\"{wav.shape=} {sample_rate=}\")\n", "display(Audio(wav, rate=sample_rate))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2023-09-16T08:21:36.443133Z", "start_time": "2023-09-16T08:21:36.129499Z"}, "cellId": "am5fprv5gj5tc0mloxg6vg", "collapsed": false, "jupyter": {"outputs_hidden": false}}, "outputs": [], "source": ["plot_wav(wav, sample_rate, end=None)"]}, {"cell_type": "markdown", "metadata": {"cellId": "2fxv0ilzujmfe4a4w2iren", "execution_id": "956fe4f7-b5ac-4ea3-82aa-d2c1750ca650", "jupyter": {"outputs_hidden": false}}, "source": ["As you can see, waveform is a redundant representation. __4-second__ audio sample where only __7 words__ are said, contains __87040__ amplitude samples! This is extremely difficult to process directly, because there's so many of them! It is far more than a typical sequence length for NLP applications. Thus, we need to compress this information to a more manageable size!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A typical solution is to use __spectrogram__: instead of saving thousands of amplitudes, we can perform [Fourier transformation](https://en.wikipedia.org/wiki/Fourier_transform) to find which periodics are prevalent at each point in time. More formally, a spectrogram applies [Short-Time Fourier Transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) to small overlapping windows of the amplitude time-series. Let us describe this pipeline and step by step"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 Let's listen to different sample rates"]}, {"cell_type": "markdown", "metadata": {"cellId": "1gjbrmf4wqu5u9tg6jvien", "execution_id": "ba1e5846-1a77-4f43-b570-b41d5bfcc82e"}, "source": ["### 2. Windowing\n", "Our speech is non-stationary and varies over time. Thus, we do not want to apply DFT to the whole recording, as it does not help us to retrieve information from the recording. What we actually want - is to see how local spectral characteristics change over time. That's why we slice our recording into overlapping windows and apply DFT to this window. This slicing has two important characteristics:\n", "- **Window_size** - the size of the window. Typically, it is 1024 or 2048 samples, which is roughly 0.05 - 0.1 seconds of audio. (Really so small)\n", "- **Hop_length** - the size of the step, between starts of two contiguous windows. Usually is equals to window_size // 4.\n", "\n", "As the hop_length is smaller than window size, the windows overlap on each other. And each sample from the wav is duplicated (window_size // hop_length) times."]}, {"cell_type": "markdown", "metadata": {"cellId": "fcpe8y1em9df569krh5vcs", "execution_id": "c16fc83a-8e0d-4141-b2c6-be0c235354cd"}, "source": ["Now you need to implement Windowing class.\n", "- Middle of $i-th$ window (element with index $(win\\_size - 1) // 2$) should lean on $(i \\times hop\\_length)-th$ of the original wav\n", "- To do that zero-pad left and right borders of the wav\n", "- The expectend number of windows is `(wav_size - window_size % 2) // hop_length + 1`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "u64c0qt3kn2xuookur9y2"}, "outputs": [], "source": ["# Assignment: implement transforms.Windowing"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "a1g21n3lxqiza33qtv9yy"}, "outputs": [], "source": ["assert test_windowing(testing_class=Windowing, n_repeats=1000)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "r031z1s2wbg7p6z685vbt2"}, "outputs": [], "source": ["start, length = 4000, 3584\n", "plot_windowing(wav[start:start + length], windowing_class=Windowing, n_subpictures=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This picture shows, how $0.16$ seconds od audio turn into several short frames, $0.09$ seconds each. The important thing here is that they intersect, quite significantly."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's apply apply the transformation to the whole wav, and see what we've got."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["windowing_transform = Windowing(window_size=2048, hop_length=512)\n", "windows = windowing_transform(wav)\n", "plot_windowed_wav(windows)\n", ""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Doesn't look like something meaningful, so let's continue doing some other transforms"]}, {"cell_type": "markdown", "metadata": {"cellId": "pvgz9meyhsf3s42mob9dl", "execution_id": "cec263e2-c392-43cc-9bfd-ed7de67bd5a6"}, "source": ["### 3. Hann window\n", "If we apply DFT to each window, resulted from the previous step, we'll get so called spectral leakage. Let me try to give you an intuition, of what it is:\n", "\n", "Fourier transform assumes, that the function is periodic and continuous.\n", "We can make function periodic, by pretending, that we loop it.\n", "But when we loop it, we break its continuousness, as the function does not match at the ends.\n", "To address this, we need our function to be equal from both sides, for example make it equal to zero.\n", "To smoothly bring the function to zero at the ends, we multiply it by hahn function elementwise.\n", "This function equals 1 in the middle, and zero at the ends (like on following picture)\n", "\n", "This guy explains that better:\n", "https://www.youtube.com/shorts/xoE2rnwFROs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Weights for Hann window can be taken [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.hann.html). "]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "1kmjkebuquzotj8jk99tw"}, "outputs": [], "source": ["# Assignment: implement transforms.Hann"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "jmi0cglw5zejlnjavk7gja"}, "outputs": [], "source": ["assert test_hann(testing_class=Hann)"]}, {"cell_type": "markdown", "metadata": {"cellId": "ocqc94np88kihrq74aqwx", "execution_id": "de1b118e-1cfe-411c-9004-a542c84eca71"}, "source": ["Let's see what we've got"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "bnj6x5k9uplubviv9octl", "tags": []}, "outputs": [], "source": ["start1, start2, length = 4500, 10450, 2048 \n", "demo_tensor = get_demo_tensor([wav[start1:start1 + length], wav[start2:start2 + length]])\n", "plot_hann(demo_tensor)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's apply our new transform to the windows we've got from the previous step."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hann_transform = Hann(window_size=2048)\n", "hann_windows = hann_transform(windows)\n", "plot_windowed_wav(hann_windows)"]}, {"cell_type": "markdown", "metadata": {"cellId": "ob5kobj1lqaf5gcvpyoybb", "execution_id": "bf7e04f0-f887-43ca-871b-d0641cc0906a"}, "source": ["### 4. FFT\n", "\n", "The Short-Time Fourier Transform (STFT) is based on the Fast Fourier Transform (FFT), which is a fast implementation of the Discrete Fourier Transform (DFT). DFT is actually what we want to do :)\n", "\n", "In numpy, there are two types of DFTs: [np.fft.fft](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html#numpy.fft.fft) and [np.fft.rfft](https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html#numpy.fft.rfft). Let's explore the differences between them."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "\n", "It's important to note that the results of these functions are complex-valued arrays.\n", "\n", "Below is a plot depicting the magnitudes and phases of both FFT and RFFT results."]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "erl8te7seke4grtcvloy1x", "tags": []}, "outputs": [], "source": ["plot_fft_difference()"]}, {"cell_type": "markdown", "metadata": {"cellId": "kk88m5zgpbo4b1pyh3mjc", "execution_id": "bbcc376f-efcd-4e93-9b1a-4f7f90231c17"}, "source": ["Let's understand, what happens on this plot.\n", "\n", "We've seen a formulae of DFT on the lecture:\n", "$$\n", "X[k] = \\sum_{n=0}^{N-1} x[n] e^{-2 \\pi i \\frac {k}{N}n}\n", "$$\n", "\n", "In this formulae $\\frac{k}{N}$ is the frequency, that we have over the x-axis.\n", "Let's deduce two important properties of this formulae.\n", "\n", "1) For complex $x(k)$\n", "$$\n", "X[N + k] = \\sum_{n=0}^{N-1} x[n] \\exp(-2 \\pi i \\frac{N + k}{N} n) = \\sum_{n=0}^{N-1} x[n] \\exp(-2 \\pi i \\frac{k}{N} n - \\cancel{2 \\pi i n}) = \\sum_{n=0}^{N-1} x[n] \\exp(-2 \\pi i \\frac{k}{N} n) \\\\\n", "X[N + k] = X[k]\n", "$$\n", "\n", "2) For real $x(k)$\n", "$$\n", "X[N - k] = \\sum_{n=0}^{N-1} x[n] \\exp(-2 \\pi i \\frac{N - k}{N} n) = \\sum_{n=0}^{N-1} x[n] \\exp(2 \\pi i \\frac{k}{N} n - \\cancel{2 \\pi i n}) = \\sum_{n=0}^{N-1} x[n] \\exp(2 \\pi i \\frac{k}{N} n) \\\\\n", "X[N - k] = X^{*}[k]\n", "$$\n", "\n", "What are the conclusions from this formulae.\n", "- The result of the formulae is periodical with period $N$. That's why if we want to calculate several values of this function for several frequencies, we can take frequencies $n$ from a range of size $N$.\n", "- In the implementation of DFT in numpy ([np.fft.fft](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html#numpy.fft.fft])) this range is chosen to be $n \\in [- \\frac{N}{2}; + \\frac{N}{2}]$, or in terms of numpy : $\\frac{n}{N} \\in [-\\frac{1}{2}; +\\frac{1}{2}]$.\n", "- We work with real-value samples as input of this function. And we want the absolutes of the results. That means, that according to formulae 2, we will get a symmetric result, $|X(n)| == |X^{*}(N - n)|$.\n", "- For this case we have another function [np.fft.rfft](https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html), which calculates the DFT for range $n \\in [0, \\frac{N}{2}]$.\n", "\n", "The same exact explanation you can see in [np.fft documentation](https://numpy.org/doc/stable/reference/routines.fft.html)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, given this knowledge, let's implement the DFT method.\n", "What is it expected to do:\n", "- Get the real-value matrix and apply DFT transformation to each of this column. \n", "- Return the real-value result.\n", "- It has `n_freqs` parameter. If it is `None`, we return all of the frequencies. If it is an integer, we return only the `n_freqs` lowest frequencies. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "4b457q3voabzjj4a7njt1h", "tags": []}, "outputs": [], "source": ["# Implement transforms.DFT"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "iaip6gsgb47qy4gr6u011", "tags": []}, "outputs": [], "source": ["assert test_dft()"]}, {"cell_type": "markdown", "metadata": {"cellId": "931x8futt7t5t1yk24dezp", "execution_id": "fe810a25-2fee-4445-8ecd-bc3c888a2b30"}, "source": ["Now let's see what we've got "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dft_transform = DFT()\n", "spec = dft_transform(hann_windows)\n", "plot_dft(spec)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.1 Let's one more time understand, what has just happened"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def signal_fn(t: float):\n", "    res = 1\n", "    res = res + 1 * np.sin(2 * np.pi * 2 * t + 0.25 * np.pi) # + 0.25 * np.pi\n", "    res = res + 2 * np.sin(2 * np.pi * 6 * t)\n", "    res = res + 0.5 * np.sin(2 * np.pi * 14 * t - 0.5 * np.pi)\n", "    return res"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's say, we have a continuous signal, like this:\n", "\n", "$$\n", "x(t) = 1 + 1 \\sin(2 \\pi \\cdot 2 t + \\frac{\\pi}{4}) + \\sin(2 \\pi \\cdot 4 t) + 0.5 \\sin(2 \\pi \\cdot 7 t - \\frac{\\pi}{2}) \\ , \\ t \\in [0, +inf)\n", "$$\n", "\n", "Our sample rate define the time between two nearest samples $T_s = \\frac{1s}{SR} \\ $, let's apply $t \\to n T_s $ \n", "\n", "$$\n", "    x[n] = 1 + 3 \\sin(2 \\pi T_s \\cdot n) + \\sin(2 \\pi T_s \\cdot 4 n) + 0.5 \\sin(2 \\pi T_s \\cdot 7 n) \\ , \\ n \\in [0, +inf)\n", "$$\n", "\n", "Then we have taken only $N$ (`window_size`) samples from this sequence. And apply a DFT transformation to it, according to the formula:\n", "\n", "$$\n", "X[k] = \\sum_{n=0}^{N-1} x[n] e^{-2 \\pi i \\frac {k}{N}n}\n", "$$\n", "According to this formula we get an array of $N$ values in time domain, and get an array of $N$ in frequency domain.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_signal_fft(signal_fn, sample_rate=128, window_size=64)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Conclusions**\n", "\n", "- Sample rate defines the Nyquist frequency. It is the maximum frequency, we can estimate with fft.\n", "- Then we choose the `window_size`, it defines the grid, at which we estimate our signal."]}, {"cell_type": "markdown", "metadata": {"cellId": "9g1z5b7rfq1150boldv7ok", "execution_id": "68e6845d-425e-47f5-96e7-acba20ca1f99"}, "source": ["### 5. Energy/Intensity/Magnitude Spectrum\n", "\n", "On the lecture, we have discussed, that for several applications we can be interested in energy, which is transmitted by given frequencies of the signal. For this purpose we can square the spectrogram.\n", "\n", "To get the loudness, we can also take the logarithm. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "fd3gk0ih8zokza2n309tl"}, "outputs": [], "source": ["plot_dft(np.log(spec ** 2))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["But we want our transformation to be inverse, so we're not going to do that"]}, {"cell_type": "markdown", "metadata": {"cellId": "tcjax0zum5kolwxjk31nrb", "execution_id": "f269c82e-8334-47e0-a252-e2a8f8dde839"}, "source": ["### 6. MelScale\n", "\n", "Initially one second of audio wav represented by 16k samples per second.\n", "We have transformed our wav in more convenient form but bitrate of the wav stayed almost the same $~(sample\\_rate \\ // \\ hop\\_length) \\times n\\_fft \\simeq 16k$ floats, representing 1 secong of audio. We won 2x space we get rid imaginary part of the signal and switched to the energy of harmonics. We lost space because overlaping of fft windows. \n", "\n", "But now we can use our knowledge of psychoacoustic as not all frequencies are useful.\n", "Different frequencies of the spectrum have different contribution for audio perception.\n", "Low frequencies are more important and high frequencies are less important as it was explained in the first lecture.\n", "We can apply to spectrogram by multiplying amplitudes on mel-scale matrix.\n", "You can see one below."]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "3r2kwh8cbi3stbidnhrs4", "tags": []}, "outputs": [], "source": ["plot_mel_scale()"]}, {"cell_type": "markdown", "metadata": {"cellId": "zxybv5wc1qp36ar5y13yr", "execution_id": "5acc680b-e993-4827-b2b6-a3b9b633d559", "tags": []}, "source": ["By multiblying each column of the spectrogram on this matrix, we reduce the number of \"frequencies\".\n", "\n", "As you can see low frequencies here have higher weight, higher frequencies - lower weights.\n", "Very high frequencies are even ignored.\n", "\n", "Another good property, that we have achieved - is that the importance of each mel bin for human comprehesion is equal.\n", "\n", "We can also condider this transformation reversible.\n", "For restoration of a spectrogram from mel, we can use pseudo-inverse matrix."]}, {"cell_type": "markdown", "metadata": {"cellId": "1aba4d9xco6i8dvnf7s0qj", "execution_id": "dc20cae9-7d73-4ca8-b5c4-3a9e23a76bcb"}, "source": ["To get the mel matrix use [this](https://librosa.org/doc/latest/generated/librosa.filters.mel.html) function with hyperparameters `fmin=1, fmax=8192`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "d5ik8q5mhls76fg983l9y"}, "outputs": [], "source": ["# Implement transforms.Mel (both __call__ and restore methods)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "065ei245oeqacxk1kwxs3t"}, "outputs": [], "source": ["test_mel(testing_class=Mel)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "3mpzn0ll70gwsmizbqyhn"}, "outputs": [], "source": ["wav_to_spec = Wav2Spectrogram(window_size=1024, hop_length=256, n_freqs=None)\n", "spec_to_mel = Mel(n_fft=1024, n_mels=80, sample_rate=22050)\n", "\n", "spec = wav_to_spec(examples[\"diesel\"])\n", "mel = spec_to_mel(spec)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "4ryfqgmh359b83irre2yp"}, "outputs": [], "source": ["plot_spec_mel(spec, mel)"]}, {"cell_type": "markdown", "metadata": {"cellId": "rn1p6ddhuuncn9d7s8nk", "execution_id": "279019a6-2e69-4ac1-9f2f-d8718da551cc"}, "source": ["Let's see how pseudo-inverse matrix restores the spectrogram"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "bjvucx9j3ug4ravg2o50w"}, "outputs": [], "source": ["restored_spec = spec_to_mel.restore(mel)\n", "plot_rerstored_spec(spec, restored_spec)"]}, {"cell_type": "markdown", "metadata": {"cellId": "ampqi0sn73enjkgip08gw", "execution_id": "8461434c-d03b-4c54-b696-74de3067c994"}, "source": ["Visually, we got pretty much the same. Yet the difference is still noticable."]}, {"cell_type": "markdown", "metadata": {"cellId": "qf9vqv0d4f39namtwyh1v", "execution_id": "84c674dc-c3fb-47c0-b730-249f4f649e94"}, "source": ["Now we got the final representation of audio, which can then be fed to a neural network, or some other algorithm.\n", "Why we like log-mel representation?\n", "\n", "- It is more compact than spectrogam\n", "- It is more informative than spectrogram\n", "- You can watch on the spectrogram and notice the basic characteristics of the audio: pitch, amount of noise, loudness and so on.\n", "\n", "What is constraints about logmel representation?\n", "\n", "- There is no phase information which important for some noise reduction algorithms\n", "- It is lossy comppression based on specific assumtion. Thus, it is not suitable for some specific scenarios like analysis of dolphins communications.\n", "\n", "For analysis of human perceptive signals it works very well. So Let's draw some specs and try to notice some insights from them"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "0woiiio26uzm7qktxt19jk"}, "outputs": [], "source": ["plot_wav_and_mel(examples)"]}, {"cell_type": "markdown", "metadata": {"cellId": "d3iag90vwj9ljr73545acs", "execution_id": "8b2552f9-9f5b-40ac-8a61-041ebb88977b"}, "source": ["### 7. Vocoding\n", "\n", "In the upcoming chapters, we will need to convert the spectrogram back to the waveform, a process known as vocoding. Logically, this involves performing the inverse discrete Fourier transform of the Discrete Fourier Transform (DFT) results. However, in many cases, we work with log-mel spectrograms of a signal and lack the phase information of the signal.\n", "\n", "\n", "The simplest way to address vocoding is GriffinLim algorithm.\n", "Shortly, uses intersection of windows to windows to estimate phases.\n", "It is implemented in Wav2Spectrogram.restore method for you.\n", "We do not cover this algorithm in our course, but if you are curious enough to figure out how it works, follow the [link](https://paperswithcode.com/method/griffin-lim-algorithm)"]}, {"cell_type": "markdown", "metadata": {"cellId": "tnwdsrihqrjyq8b05rlmii"}, "source": ["### 8. Spectrogram transformations\n", "\n", "Now, as we know how to transform wav to mel spectrogram and backward, we can implement some transformation of audio in mel space.\n", "\n", "__Assignment:__\n", "\n", "Go to the `transforms` file and mplement the following transformations over mels:\n", "\n", "* `PitchDown`, `PitchUp` - make audio sound lower and higher.\n", "* `SpeedUpDown` - increase/decrease speed of the recording by a given factor. (The same thing you do with this seminar if you watch it recorded=))\n", "* `Loudness` - increase/decrease loudness by a given factor\n", "* `TimeReverse` - inverse time\n", "* `VerticalSwap` - let's see what will happen\n", "* `WeakFrequenciesRemoval` - let's assume, that frequencies with low amplitude are not interesting for us. Let's zero frequencies smaller, than a certain quantile and see, what it changes.\n", "* `Cringe1`, `Cringe2` - use your imagination to create a some funny transfomration"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "3wnzk26vnwbm13u06lknmk"}, "outputs": [], "source": ["def show_transformation(transform_class, wav, kwargs):\n", "    sample_rate = 22050\n", "    wav2mel = Wav2Mel(\n", "        window_size=1024,\n", "        hop_length=256,\n", "        n_freqs=None,\n", "        n_mels=80,\n", "        sample_rate=sample_rate)\n", "\n", "    mel_transform = transform_class(**kwargs)\n", "\n", "    mel = wav2mel(wav)\n", "    transformed_mel = mel_transform(mel)\n", "    transformed_wav = wav2mel.restore(transformed_mel)\n", "\n", "    display(Audio(wav, rate=sample_rate))\n", "    display(Audio(transformed_wav, rate=sample_rate))\n", "    plot_transformed_mels(mel, transformed_mel)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "bbjr2neoesl7mwk0vxvcnk"}, "outputs": [], "source": ["testing_pairs = [\n", "    (PitchUp, {\"num_mels_up\": 5}),\n", "    (PitchUp, {\"num_mels_up\": 10}),\n", "    (PitchDown, {\"num_mels_down\": 5}),\n", "    (PitchDown, {\"num_mels_down\": 10}),\n", "    (SpeedUpDown, {\"speed_up_factor\": 0.5}),\n", "    (SpeedUpDown, {\"speed_up_factor\": 0.8}),\n", "    (SpeedUpDown, {\"speed_up_factor\": 1.2}),\n", "    (SpeedUpDown, {\"speed_up_factor\": 2.0}),\n", "    (Loudness, {\"loudness_factor\": 0.5}),\n", "    (Loudness, {\"loudness_factor\": 2.0}),\n", "    (TimeReverse, {}),\n", "    (VerticalSwap, {}),\n", "    (WeakFrequenciesRemoval, {\"quantile\": 0.9}),\n", "    (WeakFrequenciesRemoval, {\"quantile\": 0.99}),\n", "    (Cringe1, {}),\n", "    (Cringe2, {}),\n", "]\n", "for test_class, kwargs in testing_pairs:\n", "    print(f\"{test_class.__name__}\")\n", "    try:\n", "        show_transformation(test_class, wav, kwargs)\n", "    except NotImplementedError as e:\n", "        print(f\"Class {test_class.__name__} is not implemented yet\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {"cellId": "4py8fgogtopzv83a7afe6"}, "outputs": [], "source": [""]}], "metadata": {"kernelspec": {"display_name": "course", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.17"}, "notebookId": "6af21e1c-312c-4266-a129-fb8b6c821873", "notebookPath": "YSDA_speech_course_private/01_DSP/seminar.ipynb"}, "nbformat": 4, "nbformat_minor": 4}