{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82305e4b-70c5-4c97-a9c1-3c90b38dfa41",
   "metadata": {},
   "source": [
    "# 00. Seminar description\n",
    "\n",
    "In this seminar, you will become more familiar with the concept of pitch and implement the Monotonic Alignment Search (MAS) algorithm for estimating phoneme durations. Both of these concepts are useful for understanding where the ground truth labels for pitch and durations may come from during the training of the FastPitch architecture.\n",
    "\n",
    "The seminar consists of two parts, each of which is worth 4 points. In total, you can earn **8 points** for the seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480da877-435b-4326-a8d8-8b49f90b7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in colab\n",
    "\n",
    "# !pip install praat-parselmouth soundfile librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c29a9-339d-495b-9ff5-1f454f4eea7a",
   "metadata": {},
   "source": [
    "# 01. F0 estimation (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee96db-63c5-45cb-b61e-1fe5c59c84f9",
   "metadata": {},
   "source": [
    "F0 - the fundamental frequency of the voice (also known as pitch) was discussed in the lecture. This task involves writing one of the possible algorithms for finding F0.\n",
    "\n",
    "There is no analytically exact algorithm; proposed algorithms typically rely on a set of heuristics. Typically, the algorithm consists of three main steps:\n",
    "1. Preprocessing (signal filtering, segmentation into frames)\n",
    "2. Finding possible F0 values (candidates)\n",
    "3. Tracking - selecting the most probable F0 trajectory (since we have several competing candidates for each moment in time, we need to find the most likely track among them)\n",
    "\n",
    "## Cepstrum for F0 estimation\n",
    "\n",
    "The cepstrum is a Fourier analysis of the logarithmic amplitude spectrum of the signal:\n",
    "\n",
    "$$\n",
    "    C(x) = \\text{FFT}(\\log |\\text{FFT}(x)|)\n",
    "$$\n",
    "\n",
    "If the log amplitude spectrum contains many regularly spaced harmonics, then the Fourier analysis of the spectrum will show a peak corresponding to the spacing between the harmonics: i.e. the fundamental frequency.  \n",
    "\n",
    "Effectively we are treating the signal spectrum as another signal, then looking for periodicity in the spectrum itself.\n",
    "\n",
    "The name **cepstrum** comes from reversing the first four letters in the word “spectrum”, indicating a modified spectrum. \n",
    "\n",
    "The independent variable related to the cepstrum transform has been called **“quefrency”** (word play with \"frequency\"), and since this variable is very closely related to time it is acceptable to refer to this variable as time.\n",
    "\n",
    "Sources:  \n",
    "https://www.cs.uregina.ca/Research/Techreports/2003-06.pdf  \n",
    "https://flothesof.github.io/cepstrum-pitch-tracking.html\n",
    "\n",
    "**Note:** There are [variations in the definition of the cepstrum](https://en.wikipedia.org/wiki/Cepstrum): sometimes instead of the final FFT, the inverse FFT (iFFT) is applied, and sometimes the squares of the amplitudes are taken instead of the absolute values of the amplitudes. For our purposes, these differences in definitions are not significant.\n",
    "\n",
    "### Task\n",
    "\n",
    "We will attempt to write an algorithm that uses cepstrum to find F0.\n",
    "\n",
    "Its idea briefly consists of the following:\n",
    "1. Divide the signal into overlapping windows (similar to calculating a spectrogram), applying a Hann window to each window.\n",
    "2. In each window:\n",
    "   - Calculate the cepstrum magnitude $|C(x)|$ and the corresponding \"frequencies\" in quefrency space.\n",
    "   - We will search for F0 within the practical boundaries observed - from 80 to 450 Hz. In quefrency space, these values correspond to 1/450 and 1/80. **Food for thought:** what should be the step size in quefrency space?\n",
    "   - Choose the bin within the allowed quefrency range that corresponds to the maximum cepstrum magnitude.\n",
    "\n",
    "**Note:** The last point is heuristic; we hope that our signal contains many harmonics and that the fundamental frequency contributes the most, hence we take the maximum. Alternatively, one could try a different approach, such as taking the median of possible peaks. However, in this task, we will stick with a simple heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c684a34-a215-45a5-b500-96c9393315ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import librosa\n",
    "import parselmouth\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as Ipd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718c728-55ee-4279-ab85-6e36a0d4aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repository = ...  # Path to the repository root, e.g. /home/user/speech_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda317f3-f503-4561-91bc-ac379998fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path_to_repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22227342-bd17-466b-8797-c18d4f98bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from week_07_tts_am.fastpitch.sampled_array import SampledArray, resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29e893-6d1a-414c-ad59-275b2c83726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "WINDOW_LENGTH = 1024\n",
    "HOP_LENGTH = 256\n",
    "\n",
    "MIN_PITCH = 80\n",
    "MAX_PITCH = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c70808-e22d-4c30-812a-e9f9d5485eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spectrogram(wav: np.array) -> np.array:\n",
    "    stft = librosa.stft(\n",
    "        wav,\n",
    "        n_fft=WINDOW_LENGTH,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        win_length=WINDOW_LENGTH,\n",
    "        window='hann',\n",
    "        center=False\n",
    "    )\n",
    "    s = np.abs(stft)\n",
    "    return SampledArray(\n",
    "        value=np.log(np.maximum(1e-5, s).T),  \n",
    "        t1=0, \n",
    "        step=librosa.frames_to_time(1, sr=SAMPLE_RATE, hop_length=HOP_LENGTH)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bab80-f98a-4bee-9a91-f017dd316ab5",
   "metadata": {},
   "source": [
    "Let's look at several examples of audio with different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea8b79-605e-43b0-920b-8ae0895d9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = list((Path(path_to_repository) / 'week_07_tts_am/emotional_audios').glob('*.wav'))\n",
    "\n",
    "for path in audio_paths:\n",
    "    wav, sr = sf.read(str(path))\n",
    "    assert sr == SAMPLE_RATE\n",
    "\n",
    "    Ipd.display(Ipd.Audio(wav, rate=sr))\n",
    "\n",
    "    spec = calculate_spectrogram(wav).value.T\n",
    "    \n",
    "    max_bins = 100\n",
    "    plt.imshow(spec[:max_bins], origin=\"lower\")\n",
    "    plt.title(f'{path.name}')\n",
    "    plt.yticks([], [])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42e4c0-903b-41fb-9281-c10eb45dfdd9",
   "metadata": {},
   "source": [
    "The F0 will be located somewhere near the lowest, most prominent stripe on the spectrogram. It is evident that the contours of these stripes vary significantly for different WAV files and that extracting information about F0 could serve as valuable information, for example, in emotion recognition.  \n",
    "\n",
    "Implement the algorithm for finding F0 as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fd2f3-7adb-4f62-938b-a2fea3d19de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cepstrum(xs) -> Tuple[np.array, np.array]:\n",
    "    '''\n",
    "    Input: time signal xs\n",
    "    Return:\n",
    "        cepstrum: cepstrum values\n",
    "        quefrencies: qfrequencies (the analog of FFT frequencies, but for Cepstrum)\n",
    "\n",
    "    Useful functions:\n",
    "    - np.fft.rfft\n",
    "    - np.fft.rfftfreq\n",
    "    '''\n",
    "    <YOUR CODE HERE>\n",
    "    return cepstrum, quefrencies \n",
    "\n",
    "\n",
    "def compute_f0_for_window(cepstrum: np.array, quefrencies: np.array) -> np.array:\n",
    "    '''\n",
    "    Input:\n",
    "        cepstrum: cepstrum values\n",
    "        quefrencies: qfrequencies (the analog of FFT frequencies, but for Cepstrum)\n",
    "    Output:\n",
    "        f0: f0 value for the window (scalar)\n",
    "    '''\n",
    "    <YOUR CODE HERE>\n",
    "    return f0\n",
    "\n",
    "\n",
    "def compute_f0(wav):\n",
    "    '''\n",
    "    Computes f0 for an audio:\n",
    "    - Divides the singal into overlapping windows\n",
    "    - For each window computes f0\n",
    "    - Returns an array of each window's result\n",
    "\n",
    "    Input:\n",
    "        wav: an array of wav values\n",
    "    Output:\n",
    "        f0_values: an array of f0 values\n",
    "\n",
    "    Useful functions:\n",
    "    - librosa.util.frame\n",
    "    - np.hanning\n",
    "    '''\n",
    "    <YOUR CODE HERE>\n",
    "    return f0_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7208207-9213-48cc-ad37-db2e17b3bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = audio_paths[-1]\n",
    "wav, _  = sf.read(str(path))\n",
    "assert compute_f0(wav).shape[0] == calculate_spectrogram(wav).value.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31709b55-38bd-40ab-87a4-c81373e7a054",
   "metadata": {},
   "source": [
    "Now let's compare what we obtained with what the open-source library praat-parselmouth finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41792b9-ef88-4c76-8592-09cbb0754577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_parselmouth_pitch(path):\n",
    "    sound = parselmouth.Sound(str(path))\n",
    "    pitch_obj = sound.to_pitch(\n",
    "        pitch_floor=MIN_PITCH,\n",
    "        pitch_ceiling=MAX_PITCH\n",
    "    )\n",
    "    pitch = pitch_obj.selected_array[\"frequency\"]\n",
    "    pitch = SampledArray(value=pitch, t1=pitch_obj.t1, step=pitch_obj.time_step)\n",
    "    return pitch\n",
    "\n",
    "\n",
    "def get_freq_step():\n",
    "    freqs = np.fft.rfftfreq(n=WINDOW_LENGTH, d=1 / SAMPLE_RATE)\n",
    "    df = freqs[1] - freqs[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aeb768-c86c-459b-af31-12347aec782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bins = 100\n",
    "\n",
    "for path in audio_paths:\n",
    "    wav, sr = sf.read(str(path))\n",
    "    Ipd.display(Ipd.Audio(wav, rate=sr))\n",
    "\n",
    "    spec = calculate_spectrogram(wav)\n",
    "\n",
    "    pitch = compute_f0(wav)\n",
    "\n",
    "    assert compute_f0(wav).shape[0] == calculate_spectrogram(wav).value.shape[0]\n",
    "    \n",
    "    pitch_pm = calculate_parselmouth_pitch(str(path))\n",
    "    pitch_pm = resample(pitch_pm, spec)\n",
    "    \n",
    "    df = get_freq_step()\n",
    "    \n",
    "    plt.imshow(spec.value.T[:max_bins], origin='lower')\n",
    "    plt.scatter(x=np.arange(0, spec.value.shape[0]), y=(pitch_pm.value // df).astype(np.int64), label='parselmouth', s=1, c='r')\n",
    "    plt.scatter(x=np.arange(0, spec.value.shape[0]), y=(pitch // df).astype(np.int64), label='ours', s=1, c='white')\n",
    "    plt.legend()\n",
    "    plt.title(f'{path.name}')\n",
    "    plt.yticks([], [])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc6a52-04d9-4ef9-9d1b-6d388973051a",
   "metadata": {},
   "source": [
    "# 02. MAS for duration estimation (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6d86b-f430-4387-8d9c-cad8e2630966",
   "metadata": {},
   "source": [
    "In this task, you will need to implement MAS (Monotonic Alignment Search) to construct duration estimates. \n",
    "\n",
    "The task is to find a monotonic alignment along which the sum of log-probabilities is maximized. Such alignment can be uniquely represented as an array, where the i-th position corresponds to the number of steps for the i-th segment in the alignment. In our case, this array can be interpreted as phoneme durations: each row in the alignment corresponds to a phoneme, and the segment in the path found by MAS represents the duration of that phoneme on the spectrogram.\n",
    "\n",
    "The lecture contains references to useful articles about MAS and formulas for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb9123-1e04-47c7-bf2b-88e29cada9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mas_alignment(attention: np.ndarray):\n",
    "    \"\"\"\n",
    "     Input: attention values after applying logarithm, shape: (len(text), len(mel_spectrogram))\n",
    "    \"\"\"\n",
    "    <YOUR CODE HERE>\n",
    "    return durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a2e30-741d-438f-915f-f34ebf534bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 1, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "assert np.allclose(mas_alignment(attention), np.array([1, 2, 1]))\n",
    "\n",
    "attention = np.array([\n",
    "    [1, 0.3, 0.1, 0.2],\n",
    "    [4, 1.2, 1,     5],\n",
    "    [0, 0,   0,     1]\n",
    "])\n",
    "assert np.allclose(mas_alignment(attention), np.array([1, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28469a06-5926-471d-883b-d45e8d039148",
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_test_path = Path(path_to_repository) / 'week_07_tts_am/mas_test'\n",
    "\n",
    "alignment = np.load(mas_test_path / 'alignment.npy')\n",
    "mel = np.load(mas_test_path / 'mel.npy')\n",
    "\n",
    "wav, sr = sf.read(str(mas_test_path / 'audio.wav'))\n",
    "Ipd.display(Ipd.Audio(wav, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e3b05-4bd7-4a76-b946-d06d9810673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(mas_alignment(np.log(alignment).T), np.array([ 1,  5,  5,  6,  8,  5,  8,  7, 14,  1,  9,  1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d172d4e-87a9-4dc1-bcb0-66ba3faf93dc",
   "metadata": {},
   "source": [
    "You can try to draw the alignment and what was highlighted on the mel spectrogram. In the figure produced by the `plot_segments` function, alignment is at the bottom, mel is on top, and phonemes are in the middle. The durations of each phoneme are indicated by red lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ab2aa-2315-440c-b176-e2c209f421ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segments(mel, alignment, symbols, title=None):\n",
    "    f, ax = plt.subplots(2, 1, figsize=(7, 14), sharex=True, dpi=100)\n",
    "    ax[1].imshow(alignment.T)\n",
    "    \n",
    "    ax[0].imshow(mel)\n",
    "    ax[0].tick_params(axis=\"x\", labelsize=30)\n",
    "    plt.setp(ax[1].get_xticklabels(), visible=False)\n",
    "    \n",
    "    durations = mas_alignment(alignment)\n",
    "    durations_cs = np.hstack([[0], durations.cumsum()])\n",
    "    \n",
    "    for i, s in enumerate(durations_cs[:-1]):\n",
    "        ax[0].text(\n",
    "            s, ax[0].get_ylim()[0] * 1.15, \n",
    "            symbols[i], \n",
    "            rotation=90, fontsize=18\n",
    "        )\n",
    "    \n",
    "    for s in durations_cs:\n",
    "        ax[0].axvline(x=s, c='r')\n",
    "        ax[1].axvline(x=s, c='r')\n",
    "\n",
    "    f.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633992c-cd64-45c8-a257-3d06e424319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['BOS', 'a', 'nn', 'tt', 'i', 'l', 'oo', 'p', 'schwa', ' ', '.', 'EOS']\n",
    "\n",
    "plot_segments(\n",
    "    mel, \n",
    "    alignment, \n",
    "    symbols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21804ed1-0c65-44da-90b4-d058db25ad1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpitch",
   "language": "python",
   "name": "fastpitch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
